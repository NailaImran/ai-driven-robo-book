---
title: Sensor Fusion
description: Combining multiple sensor modalities for robust robot perception
sidebar_position: 4
---

# Sensor Fusion

Learn how to combine data from multiple sensors to create a more accurate and robust understanding of the robot's environment.

---

## Why Sensor Fusion?

No single sensor is perfect. Each has strengths and weaknesses:

| Sensor | Strengths | Weaknesses |
|--------|-----------|------------|
| **Camera** | Rich visual information, texture, color | Poor in darkness, sensitive to lighting |
| **LiDAR** | Accurate distance, works in darkness | No color/texture, expensive, heavy |
| **IMU** | High-frequency orientation | Drift over time, no position |
| **GPS** | Absolute global position | Poor indoors, 3-10m accuracy |
| **Wheel Odometry** | Smooth, high-frequency | Drift from wheel slip |

**Sensor fusion** combines complementary sensors to overcome individual limitations.

---

## Fusion Architectures

### 1. Complementary Fusion

Different sensors measure different quantities:

```python
# Example: Combining camera (objects) + LiDAR (distance)
def complementary_fusion(camera_detection, lidar_scan):
    """
    Camera detects WHAT object is present.
    LiDAR measures HOW FAR it is.
    Together: 3D localization of objects.
    """
    object_class = camera_detection.class_name  # "person"
    bearing = camera_detection.center_x  # Direction in image

    # Find LiDAR point at same bearing
    distance = lidar_scan.ranges[bearing]

    # Combine: "Person at 2.5m, 30° to the left"
    return Object3D(
        class_name=object_class,
        distance=distance,
        bearing=bearing
    )
```

### 2. Redundant Fusion

Multiple sensors measure the same quantity for robustness:

```python
# Example: Combining wheel odometry + IMU + visual odometry
def redundant_fusion(wheel_odom, imu, visual_odom):
    """
    All three estimate robot velocity.
    Fuse with weights based on reliability.
    """
    # Wheel odometry: unreliable on slippery surfaces
    # IMU: reliable for rotation, drifts for position
    # Visual odometry: fails in featureless environments

    if is_slipping():
        weights = [0.1, 0.3, 0.6]  # Trust vision more
    elif is_dark():
        weights = [0.5, 0.4, 0.1]  # Trust wheels and IMU
    else:
        weights = [0.4, 0.3, 0.3]  # Balanced

    velocity = (
        weights[0] * wheel_odom.velocity +
        weights[1] * imu.velocity +
        weights[2] * visual_odom.velocity
    )

    return velocity
```

### 3. Cooperative Fusion

Sensors help each other:

```python
# Example: IMU-aided camera tracking
def cooperative_fusion(camera_frame, imu_data):
    """
    IMU predicts camera motion between frames.
    Camera corrects IMU drift.
    """
    # IMU integration predicts next pose
    predicted_pose = integrate_imu(imu_data, dt=0.033)

    # Use prediction to guide visual tracking
    tracked_pose = track_features(
        camera_frame,
        initial_guess=predicted_pose  # Faster convergence
    )

    # Visual pose corrects IMU bias
    imu_bias_correction = tracked_pose - predicted_pose

    return tracked_pose, imu_bias_correction
```

---

## Kalman Filter: The Classic Approach

The **Kalman Filter** is the most common sensor fusion algorithm for linear systems.

### Intuition

Imagine you're trying to locate a robot:
- **GPS** says: "You're at (10, 20) ± 5m error"
- **Wheel odometry** says: "You moved 2m forward from last position"

The Kalman Filter **optimally combines** both estimates based on their uncertainties.

### Math Foundations

**State**: Robot position and velocity
```
x = [position_x, position_y, velocity_x, velocity_y]
```

**Prediction Step** (using motion model):
```
x_predicted = F * x_previous + B * u + w
```
- `F`: State transition matrix (physics)
- `u`: Control input (motor commands)
- `w`: Process noise (uncertainty in model)

**Update Step** (using sensor measurement):
```
x_updated = x_predicted + K * (z - H * x_predicted)
```
- `z`: Sensor measurement
- `H`: Measurement matrix (maps state to observation)
- `K`: Kalman gain (optimal weight between prediction and measurement)

### Implementation Example

```python
import numpy as np

class KalmanFilter:
    def __init__(self):
        # State: [x, y, vx, vy]
        self.x = np.zeros(4)

        # Covariance matrix (uncertainty)
        self.P = np.eye(4) * 1000  # High initial uncertainty

        # Process noise (model uncertainty)
        self.Q = np.eye(4) * 0.01

        # Measurement noise (sensor uncertainty)
        self.R_gps = np.eye(2) * 25  # GPS: 5m std dev
        self.R_odom = np.eye(2) * 1  # Odometry: 1m std dev

    def predict(self, dt, velocity_command):
        """Prediction step using motion model"""
        # State transition matrix (constant velocity model)
        F = np.array([
            [1, 0, dt, 0],
            [0, 1, 0, dt],
            [0, 0, 1, 0],
            [0, 0, 0, 1]
        ])

        # Control input matrix
        B = np.array([
            [0, 0],
            [0, 0],
            [dt, 0],
            [0, dt]
        ])

        # Predict state
        self.x = F @ self.x + B @ velocity_command

        # Predict covariance
        self.P = F @ self.P @ F.T + self.Q

    def update_gps(self, gps_measurement):
        """Update step with GPS measurement"""
        # Measurement matrix (GPS measures position only)
        H = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ])

        # Innovation (measurement residual)
        y = gps_measurement - H @ self.x

        # Innovation covariance
        S = H @ self.P @ H.T + self.R_gps

        # Kalman gain
        K = self.P @ H.T @ np.linalg.inv(S)

        # Update state
        self.x = self.x + K @ y

        # Update covariance
        self.P = (np.eye(4) - K @ H) @ self.P

    def update_odom(self, odom_measurement):
        """Update step with odometry measurement"""
        # Similar to GPS but with different R matrix
        H = np.array([
            [1, 0, 0, 0],
            [0, 1, 0, 0]
        ])

        y = odom_measurement - H @ self.x
        S = H @ self.P @ H.T + self.R_odom
        K = self.P @ H.T @ np.linalg.inv(S)

        self.x = self.x + K @ y
        self.P = (np.eye(4) - K @ H) @ self.P

    def get_position(self):
        """Return current position estimate"""
        return self.x[:2]

    def get_uncertainty(self):
        """Return position uncertainty (standard deviation)"""
        return np.sqrt(np.diag(self.P[:2, :2]))

# Usage
kf = KalmanFilter()

for timestep in range(100):
    dt = 0.1  # 10 Hz

    # Get sensor data
    velocity_cmd = get_motor_commands()  # [vx, vy]
    gps = get_gps_measurement()  # [x, y]
    odom = get_odometry()  # [x, y]

    # Prediction
    kf.predict(dt, velocity_cmd)

    # Update with sensors
    if gps_available():
        kf.update_gps(gps)

    if odom_available():
        kf.update_odom(odom)

    # Get fused estimate
    position = kf.get_position()
    uncertainty = kf.get_uncertainty()

    print(f"Position: ({position[0]:.2f}, {position[1]:.2f}) ± {uncertainty[0]:.2f}m")
```

---

## Extended Kalman Filter (EKF)

For **nonlinear** systems (most real robots), we use the Extended Kalman Filter.

### When to Use EKF

Use EKF when:
- Robot motion is nonlinear (e.g., differential drive turning)
- Sensor measurements are nonlinear (e.g., bearing-only sensors)
- Working with angles (rotation is nonlinear)

### Key Difference from KF

EKF **linearizes** nonlinear functions using Jacobian matrices.

```python
# Instead of: x_pred = F * x
# We use: x_pred = f(x, u)  # Nonlinear function
# And linearize: F = ∂f/∂x (Jacobian)
```

### Example: Robot Localization with EKF

```python
import numpy as np

class ExtendedKalmanFilter:
    def __init__(self):
        # State: [x, y, theta]
        self.x = np.zeros(3)
        self.P = np.eye(3) * 10

        self.Q = np.diag([0.1, 0.1, 0.05])  # Process noise
        self.R = np.diag([1.0, 1.0])  # Measurement noise

    def predict(self, velocity, angular_velocity, dt):
        """Nonlinear prediction step"""
        x, y, theta = self.x

        # Nonlinear motion model (differential drive)
        if abs(angular_velocity) < 1e-6:
            # Straight line motion
            x_new = x + velocity * np.cos(theta) * dt
            y_new = y + velocity * np.sin(theta) * dt
            theta_new = theta
        else:
            # Curved motion
            radius = velocity / angular_velocity
            dtheta = angular_velocity * dt

            x_new = x + radius * (np.sin(theta + dtheta) - np.sin(theta))
            y_new = y - radius * (np.cos(theta + dtheta) - np.cos(theta))
            theta_new = theta + dtheta

        self.x = np.array([x_new, y_new, theta_new])

        # Jacobian of motion model
        F = np.array([
            [1, 0, -velocity * np.sin(theta) * dt],
            [0, 1, velocity * np.cos(theta) * dt],
            [0, 0, 1]
        ])

        # Predict covariance
        self.P = F @ self.P @ F.T + self.Q

    def update_landmark(self, landmark_position, measured_range, measured_bearing):
        """Update with landmark observation"""
        lx, ly = landmark_position
        x, y, theta = self.x

        # Expected measurement (nonlinear)
        dx = lx - x
        dy = ly - y
        expected_range = np.sqrt(dx**2 + dy**2)
        expected_bearing = np.arctan2(dy, dx) - theta

        # Jacobian of measurement model
        H = np.array([
            [-dx/expected_range, -dy/expected_range, 0],
            [dy/(dx**2 + dy**2), -dx/(dx**2 + dy**2), -1]
        ])

        # Innovation
        y_range = measured_range - expected_range
        y_bearing = self.normalize_angle(measured_bearing - expected_bearing)
        y = np.array([y_range, y_bearing])

        # Standard Kalman update
        S = H @ self.P @ H.T + self.R
        K = self.P @ H.T @ np.linalg.inv(S)

        self.x = self.x + K @ y
        self.x[2] = self.normalize_angle(self.x[2])  # Keep angle in [-π, π]

        self.P = (np.eye(3) - K @ H) @ self.P

    @staticmethod
    def normalize_angle(angle):
        """Wrap angle to [-π, π]"""
        while angle > np.pi:
            angle -= 2 * np.pi
        while angle < -np.pi:
            angle += 2 * np.pi
        return angle
```

---

## Particle Filter: Handling Multi-Modal Distributions

When the robot is **lost** or facing **ambiguous situations**, the Kalman Filter fails (it assumes Gaussian distributions). **Particle Filters** can handle multiple hypotheses.

### Concept

Represent the belief as **thousands of particles** (possible robot states):

```python
# 1000 possible robot poses
particles = [
    Pose(x=10.2, y=5.1, theta=0.3),
    Pose(x=10.1, y=5.0, theta=0.29),
    Pose(x=15.5, y=8.2, theta=1.2),  # Alternative hypothesis
    ...
]
```

Each particle has a **weight** representing how likely it is given sensor measurements.

### Algorithm

```python
class ParticleFilter:
    def __init__(self, num_particles=1000):
        # Initialize particles randomly
        self.particles = []
        for _ in range(num_particles):
            self.particles.append(Pose(
                x=np.random.uniform(0, 100),
                y=np.random.uniform(0, 100),
                theta=np.random.uniform(-np.pi, np.pi)
            ))
        self.weights = np.ones(num_particles) / num_particles

    def predict(self, velocity, angular_velocity, dt):
        """Move each particle according to motion model + noise"""
        for particle in self.particles:
            # Add noise to motion
            v_noisy = velocity + np.random.normal(0, 0.1)
            w_noisy = angular_velocity + np.random.normal(0, 0.05)

            # Update particle pose
            if abs(w_noisy) < 1e-6:
                particle.x += v_noisy * np.cos(particle.theta) * dt
                particle.y += v_noisy * np.sin(particle.theta) * dt
            else:
                r = v_noisy / w_noisy
                dtheta = w_noisy * dt

                particle.x += r * (np.sin(particle.theta + dtheta) - np.sin(particle.theta))
                particle.y += -r * (np.cos(particle.theta + dtheta) - np.cos(particle.theta))
                particle.theta += dtheta

    def update(self, measurement, sensor_model):
        """Reweight particles based on measurement likelihood"""
        for i, particle in enumerate(self.particles):
            # Compute how likely this measurement is if particle is true pose
            self.weights[i] = sensor_model.likelihood(measurement, particle)

        # Normalize weights
        self.weights /= np.sum(self.weights)

    def resample(self):
        """Resample particles (remove unlikely, duplicate likely)"""
        indices = np.random.choice(
            len(self.particles),
            size=len(self.particles),
            replace=True,
            p=self.weights
        )

        self.particles = [self.particles[i] for i in indices]
        self.weights = np.ones(len(self.particles)) / len(self.particles)

    def get_estimate(self):
        """Return weighted average of particles"""
        x = np.sum([p.x * w for p, w in zip(self.particles, self.weights)])
        y = np.sum([p.y * w for p, w in zip(self.particles, self.weights)])
        theta = np.arctan2(
            np.sum([np.sin(p.theta) * w for p, w in zip(self.particles, self.weights)]),
            np.sum([np.cos(p.theta) * w for p, w in zip(self.particles, self.weights)])
        )

        return Pose(x, y, theta)

# Usage
pf = ParticleFilter(num_particles=500)

while True:
    # Prediction
    pf.predict(velocity, angular_velocity, dt)

    # Update with measurement
    laser_scan = get_lidar_scan()
    pf.update(laser_scan, lidar_sensor_model)

    # Resample (every 5 steps)
    if step % 5 == 0:
        pf.resample()

    # Get best estimate
    estimated_pose = pf.get_estimate()
```

---

## ROS 2 Sensor Fusion with robot_localization

The `robot_localization` package fuses odometry, IMU, and GPS using EKF/UKF.

### Installation

```bash
sudo apt install ros-humble-robot-localization
```

### Configuration

**ekf_config.yaml**:
```yaml
ekf_localization_node:
  ros__parameters:
    frequency: 30.0
    sensor_timeout: 0.1
    two_d_mode: true  # For ground robots

    # Coordinate frames
    map_frame: map
    odom_frame: odom
    base_link_frame: base_link
    world_frame: odom

    # Input sources
    odom0: /wheel_odometry
    odom0_config: [true,  true,  false,   # x, y, z
                   false, false, true,    # roll, pitch, yaw
                   true,  true,  false,   # vx, vy, vz
                   false, false, true,    # vroll, vpitch, vyaw
                   false, false, false]   # ax, ay, az

    imu0: /imu/data
    imu0_config: [false, false, false,    # x, y, z
                  true,  true,  true,     # roll, pitch, yaw (from IMU)
                  false, false, false,    # vx, vy, vz
                  false, false, false,    # vroll, vpitch, vyaw
                  true,  true,  true]     # ax, ay, az (from IMU)

    # Process noise covariance
    process_noise_covariance: [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0.06, 0,     0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0.025, 0,     0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0.025, 0,    0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0.02, 0,    0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0.01, 0,    0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0.01, 0,
                               0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0.015]
```

### Launch

```python
# sensor_fusion.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
import os

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='robot_localization',
            executable='ekf_node',
            name='ekf_localization',
            output='screen',
            parameters=[os.path.join(
                get_package_share_directory('my_robot'),
                'config',
                'ekf_config.yaml'
            )]
        )
    ])
```

---

## Practical Tips

### 1. Sensor Calibration

Always calibrate sensors before fusion:

```python
# IMU calibration: measure bias when stationary
imu_bias = np.mean(imu_readings_at_rest, axis=0)
calibrated_imu = raw_imu - imu_bias

# Camera-LiDAR calibration: find transformation matrix
T_camera_to_lidar = calibrate_extrinsics(checkerboard_images, lidar_scans)
```

### 2. Outlier Rejection

Filter out bad measurements:

```python
def is_outlier(measurement, previous_estimate, threshold=3.0):
    """Reject measurements too far from prediction (Mahalanobis distance)"""
    innovation = measurement - predicted_measurement
    innovation_covariance = H @ P @ H.T + R

    mahalanobis_dist = np.sqrt(innovation.T @ np.linalg.inv(innovation_covariance) @ innovation)

    return mahalanobis_dist > threshold
```

### 3. Asynchronous Sensor Fusion

Sensors arrive at different rates. Buffer and interpolate:

```python
class AsynchronousFusion:
    def __init__(self):
        self.latest_estimate = None
        self.sensor_buffer = {}

    def add_measurement(self, sensor_name, timestamp, data):
        """Add measurement to buffer"""
        if sensor_name not in self.sensor_buffer:
            self.sensor_buffer[sensor_name] = []

        self.sensor_buffer[sensor_name].append((timestamp, data))

        # Fuse when all sensors have recent data
        if self.all_sensors_ready():
            self.fuse()

    def fuse(self):
        """Combine buffered measurements"""
        # Interpolate to common timestamp
        common_time = max([buf[-1][0] for buf in self.sensor_buffer.values()])

        measurements = {}
        for sensor, buffer in self.sensor_buffer.items():
            measurements[sensor] = self.interpolate(buffer, common_time)

        # Run fusion algorithm
        self.latest_estimate = self.kalman_filter.update(measurements)

        # Clear old data
        self.clear_old_measurements(common_time)
```

---

## Summary

| Method | Best For | Pros | Cons |
|--------|----------|------|------|
| **Kalman Filter** | Linear systems, Gaussian noise | Optimal for linear case, fast | Assumes linearity, single mode |
| **Extended KF** | Mildly nonlinear systems | Handles nonlinearity | Linearization errors, single mode |
| **Particle Filter** | Highly nonlinear, multi-modal | No Gaussian assumption, robust | Computationally expensive |
| **robot_localization** | ROS 2 robots, standard sensors | Easy to use, well-tested | Limited to odometry/IMU/GPS |

---

## Exercises

1. **Implement 1D Kalman Filter**: Fuse GPS and odometry for 1D position tracking
2. **EKF Localization**: Implement robot localization with range-bearing landmarks
3. **Particle Filter**: Build global localization system (robot kidnapping problem)
4. **ROS 2 Fusion**: Configure `robot_localization` for TurtleBot3

---

## Next Steps

- [**SLAM Practice**](./slam-practice.mdx): Build complete SLAM systems
- [**Week 8: NVIDIA Isaac Platform**](/docs/04-nvidia-isaac/): GPU-accelerated perception

**Further Reading**:
- [Probabilistic Robotics](http://www.probabilistic-robotics.org/) - Thrun, Burgard, Fox
- [robot_localization Documentation](http://docs.ros.org/en/humble/p/robot_localization/)
- [Kalman Filter Explained Visually](https://www.kalmanfilter.net/)
